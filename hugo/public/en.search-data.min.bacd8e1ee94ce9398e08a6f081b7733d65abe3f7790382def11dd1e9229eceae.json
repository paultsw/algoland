[{"id":0,"href":"/docs/dsa/trees_and_heaps/binary_heaps_priority_queues/","title":"Binary Heaps and Priority Queues","section":"Trees and Heaps","content":" what\u0026rsquo;s a binary heap? # TODO\nwhat\u0026rsquo;s a priority queue? # TODO\nimplementing pq\u0026rsquo;s with heaps # TODO\n"},{"id":1,"href":"/docs/dsa/probabilistic/bloom_filters/","title":"Bloom Filters","section":"Probabilistic","content":" probabilistic retrieval from a fuzzy set # Bloom filters are motivated by the following question: how do we use a fixed amount of memory to check set-inclusion?\nAs always there are tradeoffs. The tradeoff in this case is that we lose certainty \u0026mdash; but what in life is certain, anyway?\nMore formally, a bloom filter is a probabilistic data structure \u0026mdash; a data structure that \u0026mdash; which implements the following operations:\ninsert(x) \u0026mdash; inidicate that an item x has been added into the set. query(x) \u0026mdash; check to see if an item x has been inserted into the bloom filter previously. Notably, the standard implementation of a Bloom filter does not implement an operation to remove an element from the structure, since erasing an item\u0026rsquo;s inclusion from the set would also corrupt the inclusion queries for other items.\nimplementing a bloom filter with hash functions # How do we implement Bloom filters? The short answer is with hash functions that map elements to addresses of a memory bank of bits.\nImagine you have m bits labeled 1 to m, and k hash functions $$ h_k: X \\to \\{1,\\ldots,m\\} $$ so that there is little-to-no correlation between each hash function (this is important).\nThen the way we insert an element x is to compute all the hash values $$ H(x) = \\{ h_1(x), \\ldots, h_k(x) \\} \\subset [m] $$ and flip all of their bits from 0 to 1.\nWhen we query for the existence of x in the set represented by the Bloom filter, we compute H(x) again, and check that all the bits are equal to 1; if there is even a single zero bit in the hashed bits H(x), we return false for the query.\nfalse positive rates # It\u0026rsquo;s possible to get a false positive with Bloom filters (the filter says x is in the set, but it was never actually inserted).\nIt\u0026rsquo;s not possible to get false negatives (the filter says x is not in the set, but it\u0026rsquo;s actually been inserted).\nWhat\u0026rsquo;s the chance of getting one of these false positives? Well, if we\u0026rsquo;re using k hash functions per element across m bits, we can do some basic math to find that the probability of a collision after inserting n elements is: $$ \\biggl( 1 - \\exp\\frac{-kn}{m} \\biggr)^k $$\noptimal settings for bloom filters # Say we have a fixed budget of $m$ elements in our memorybank. How many hash functions should we be using to minimize the false positive rate?\n(Watch this space; this is a topic for a future update.)\nimplementing it in python # In the below, we assume we can construct a set of hash functions based on big prime numbers and modular arithmetic; see this link for the complicated details of how that\u0026rsquo;s possible. But in general, designing hash functions is hard, and it\u0026rsquo;s a vibrant field of algorithmics research to design excellent hash functions for various purposes \u0026mdash; for instance, cryptographic security, low collision rates, et cetera.\nclass BloomFilter(object): \u0026#34;\u0026#34;\u0026#34; Bloom filter implementation. \u0026#34;\u0026#34;\u0026#34; def __init__(self, numhashes, numbits): self.numhashes = numhashes self.numbits = numbits self.hash_functions = [ ] for k in range(numhashes): hk = (lambda x: COEFF[k]*x + OFFSET[k] % BIGPRIME[k]) self.hash_functions.append(hk) self.memory = [0 for _ in range(numbits)] def insert(self, x): bits = [ hf(x) for hf in self.hash_functions ] for bit in bits: self.memory[bit] = 1 def query(self, x): bits = [ hf(x) for hf in self.hash_functions ] return all([self.memory[b] == 1 for b in bits]) "},{"id":2,"href":"/docs/ml/theory/probability_distributions/","title":"Common probability distributions","section":"Theory","content":" Some common probability distributions and their properties # uniform (discrete and continuous) # bernoulli # binomial # normal (gaussian) # student-t # multinoulli # categorical # dirichlet # gumbel # type-1 gumbel # type-2 gumbel # weibull # chi-squared # logistic # laplace # cauchy # log-normal # pareto # gamma # special case of the gamma: exponential distribution # special case of the gamma: beta distribution # poisson # multivariate normal distribution # multivariate student t # "},{"id":3,"href":"/docs/dsa/trees_and_heaps/fibonacci_heaps/","title":"Fibonacci Heaps","section":"Trees and Heaps","content":" fibonacci heaps # TODO\n"},{"id":4,"href":"/docs/dsa/graphs/min_span_trees/","title":"Min Span Trees","section":"Graphs","content":" minimum spanning trees # Say you have a graph G. A minimum spanning tree (or MST, or min span tree) is a ...TODO...\nkruskal\u0026rsquo;s algorithm # Kruskal\u0026rsquo;s algorithm builds an MST by ...\nprim\u0026rsquo;s algorithm # Compared to Kruskal, Prim\u0026rsquo;s algorithm uses ...\nboruvka\u0026rsquo;s algorithm # Now we\u0026rsquo;re getting deep into the weeds here. ...\n"},{"id":5,"href":"/docs/dsa/graphs/shortest_paths/","title":"Shortest Paths","section":"Graphs","content":" shortest path problems # Given a graph G, you often want to find the shortest path from one node to another. TODO: elaborate here\ndijkstra\u0026rsquo;s algorithm # The classic.\nfloyd-warshall # TODO\nbellman-ford # TODO\n"},{"id":6,"href":"/docs/dsa/trees_and_heaps/treaps/","title":"Treaps","section":"Trees and Heaps","content":" what are treaps? # TODO\n"},{"id":7,"href":"/posts/intro-algoland/","title":"Welcome to Algoland!","section":"Posts","content":" What\u0026rsquo;s all this then? # Just a big ole repository of algorithms and techniques from classical data structures and algorithms, some advanced DSA, and some machine learning techniques.\n"},{"id":8,"href":"/docs/dsa/","title":"Data Structures and Algorithms","section":"Docs","content":" algorithms # Algorithms are recipes for your computer. They are recipes, usually implemented on a computer following a Turing machine (or a Von Neumann architectured machine), but can also refer to those that are done on an abstract lambda calculus.\nIn particular, algorithms are usually clever recipes implemented to solve a particular problem. This can involve things like: how to count the number of ways to make change for a certain number of dollars; how to find the fastest route from start to finish in a given topology; or how to arrange a set of jumbled numbers into an orderly sequence.\ndata structures # Data structures, on the other hand, are abstract structures that allow us to store, retrieve, and manipulate data in convenient ways.\nWhat we mean by \u0026ldquo;convenient\u0026rdquo; can vary \u0026mdash; maybe it means we can do something quickly. Maybe it means we can yank out a certain element of the dataset quickly. Or maybe it means we can add something to a collection quickly.\nData structures are closely related to algorithms. For instance, we might need a clever algorithm to set up a data structure in a meaningfully quick way. Or maybe a data structure relies on a clever algorithm to retrieve or modify an existing data element.\nData structures come in many many flavors:\nmaybe we\u0026rsquo;re dealing with a tree, which is a data structure that has a hierarchical quality to it; this can enable, for instance, fast lookups if we have a balanced binary tree. graphs are data structures and can be represented on a computer in many ways; they represent datasets where the individual elements have some sort of connection with other elements. We have randomized, or _probabilistic data structures: structures that don\u0026rsquo;t offer a hard-and-fast guarantee of good performance, but guarantee good performance \u0026ldquo;almost all the time\u0026rdquo;, by using randomness in the underlying methods. Another popular structure is the hash table, or just table. These rely on the properties of certain mathematical functions, called hash functions, to provide fast lookups. "},{"id":9,"href":"/docs/ml/","title":"Machine Learning","section":"Docs","content":" all about machine learning # Machine learning is \u0026ldquo;the study and development of statistical algorithms that can learn from data and generalize to unseen data\u0026rdquo;. (Yes, I stole that from wikipedia.)\nToday, the collective attention1 of the field is focused on deep learning \u0026mdash; the technique that\u0026rsquo;s taken the subject by storm ever since results showed that they could be highly-scalable with respect to data and compute. But ML spans far beyond that, and includes classical methods like clustering, tree-based models, and mathematical techniques like integer programming and planning methods (e.g., computational methods based on Bellman\u0026rsquo;s equation).\nThis overview introduces a few of the major components underpinning modern ML: linear algebra and probability theory. In the sub-entries, we\u0026rsquo;ll dig a bit deeper.\nlinear algebra # Linear algebra is one of those rare fields of mathematics where (almost) everything is fully understood. It is the theory behind vectors, matrices, and multidmensional (rigid) geometry.\nprobability theory # Probability theory and statistics is just as fundamental. It is the mathematical description of randomness, of different \u0026ldquo;versions\u0026rdquo; of randomness (i.e. different probability distributions) arising from different random processes, and the study of how these differences lead to differing sampled outcomes. Inferential statistics discusses how to recover probability distributions, or certain aspects of probability distributions, from a finite number of samples.\nBa dum tishhhhh. Yes, this is a joke.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"}]